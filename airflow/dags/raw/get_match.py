from airflow.utils.task_group import TaskGroup
import logging
import time
from datetime import datetime
import requests
import pandas as pd
from bs4 import BeautifulSoup
from airflow.utils.dates import days_ago
import re

import sys
import os
sys.path.insert(0, '/opt/airflow/dags/')
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from variables_settings import variables, base_getmatch
from raw.base_job_parser import BaseJobParser

table_name = variables['raw_tables'][6]['raw_tables_name']
BASE_URL = base_getmatch

logging.basicConfig(
    format='%(threadName)s %(name)s %(levelname)s: %(message)s',
    level=logging.INFO
)

log = logging

# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
default_args = {
    "owner": "admin_1T",
    'start_date': days_ago(1)
}

class GetMatchJobParser(BaseJobParser):
    def find_vacancies(self):
        """
        This method parses job vacancies from the GetMatch website.
        It retrieves the vacancy details such as company name, vacancy name, skills required, location, job format,
        salary range, date created, and other relevant information.
        The parsed data is stored in a DataFrame for further processing.
        """
        # i = 1
        # # BASE_URL = 'https://getmatch.ru/api/offers?sa=150000&p={i}&pa=all&s=landing_ca_header&offset=20&limit=20'
        HEADERS = {
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8",
            "User-Agent": "Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:98.0) Gecko/20100101 Firefox/98.0",
        }
        self.log.info(f'–°–æ–∑–¥–∞–µ–º –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫')
        self.items = []
        self.all_links = []
        self.log.info(f'–ü–∞—Ä—Å–∏–º –¥–∞–Ω–Ω—ã–µ')
        try:
            # –ü–∞—Ä—Å–∏–º –ª–∏–Ω–∫–∏ –≤–∞–∫–∞–Ω—Å–∏–π —Å –∫–∞–∂–¥–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã (–Ω–∞ —Å–∞–π—Ç–µ –ø—Ä–∏–º–µ—Ä–Ω–æ 50—Å—Ç—Ä–∞–Ω–∏—Ü, 100 - —ç—Ç–æ —Å –∏–∑–±—ã—Ç–∫–æ–º)
            for i in range(1, 100):
                url = BASE_URL.format(i=i)  # –û–±–Ω–æ–≤–ª—è–µ–º URL –Ω–∞ –∫–∞–∂–¥–æ–π –∏—Ç–µ—Ä–∞—Ü–∏–∏
                r = requests.get(url)
                if r.status_code == 200:
                    # –ü–∞—Ä—Å–∏–º JSON-–æ—Ç–≤–µ—Ç
                    data = r.json()
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å—Å—ã–ª–∫–∏ –∏–∑ JSON
                    for job in data.get('offers', []):
                        # –ü–æ–ª—É—á–∞–µ–º –¥–∞—Ç—É —Ä–∞–∑–º–µ—â–µ–Ω–∏—è –∏–∑ JSON
                        date_created = job.get('published_at')
                        # –ü–æ–ª—É—á–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ –≤–∞–∫–∞–Ω—Å–∏–∏ –∏–∑ JSON
                        vacancy_name = job.get('position')
                        # –ü–æ–ª—É—á–∞–µ–º –∑–∞—Ä–ø–ª–∞—Ç—É –∏–∑ JSON
                        —Åurr_salary_from = —Åurr_salary_to = salary_from = salary_to = currency_id = None
                        salary = job.get('salary_description')
                        if salary:
                            salary_text = salary.replace('\u200d', '-').replace('‚Äî', '-')
                            salary_parts = list(map(str.strip, salary_text.split('-')))
                            # –ï—Å–ª–∏ –∑–∞—Ä–ø–ª–∞—Ç–∞ —Ä—É–±–ª–µ–≤–∞—è
                            if '‚ÇΩ' in salary or '‚Ç¨' in salary or '$' in salary or '‚Ç∏' in salary:
                                —Åurr_salary_from = salary_parts[0]
                                if len(salary_parts) == 1:
                                    —Åurr_salary_to = None if '–æ—Ç' in salary else salary_parts[0]
                                elif len(salary_parts) > 1:
                                    —Åurr_salary_to = salary_parts[2]
                            if '‚Ç¨' in salary:
                                currency_id = 'EUR'
                            elif '$' in salary:
                                currency_id = 'USD'
                            elif '‚Ç∏' in salary:
                                currency_id = 'KZT'
                            elif '‚ÇΩ' in salary:
                                currency_id = 'KZT'
                            # –ü—Ä–∏–≤–æ–¥–∏–º –∑–∞—Ä–ø–ª–∞—Ç—ã –∫ —á–∏—Å–ª–æ–≤–æ–º—É —Ñ–æ—Ä–º–∞—Ç—É
                            #     if salary_from is not None:
                            #         numbers = re.findall(r'\d+', salary_from)
                            #         combined_number = ''.join(numbers)
                            #         salary_from = int(combined_number) if combined_number else None
                            #     if salary_to is not None:
                            #         numbers = re.findall(r'\d+', salary_to)
                            #         combined_number = ''.join(numbers)
                            #         salary_to = int(combined_number) if combined_number else None
                            if —Åurr_salary_from is not None:
                                numbers = re.findall(r'\d+', —Åurr_salary_from)
                                combined_number = ''.join(numbers)
                                —Åurr_salary_from = int(combined_number) if combined_number else None
                            if —Åurr_salary_to is not None:
                                numbers = re.findall(r'\d+', —Åurr_salary_to)
                                combined_number = ''.join(numbers)
                                —Åurr_salary_to = int(combined_number) if combined_number else None
                        # –ü–æ–ª—É—á–∞–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ –≤–∞–∫–∞–Ω—Å–∏–∏ –∏–∑ JSON
                        description_text = job.get('offer_description')
                        description = BeautifulSoup(description_text, 'html.parser').get_text()

                        # –ü–æ–ª—É—á–∞–µ–º —Å—Å—ã–ª–∫—É –Ω–∞ –≤–∞–∫–∞–Ω—Å–∏—é –∏–∑ JSON
                        full_url = 'https://getmatch.ru' + str(job.get('url'))

                        # –ü–µ—Ä–µ—Ö–æ–¥–∏–º –ø–æ —Å—Å—ã–ª–∫–µ, —á—Ç–æ–±—ã —Å–ø–∞—Ä—Å–∏—Ç—å –æ—Å—Ç–∞–≤—à–∏–µ—Å—è —ç–ª–µ–º–µ–Ω—Ç—ã
                        resp = requests.get(full_url, HEADERS)
                        vac = BeautifulSoup(resp.content, 'lxml')
                        try:
                            # –ü–∞—Ä—Å–∏–º —É—Ä–æ–≤–µ–Ω—å
                            term_element = vac.find('div', {'class': 'col b-term'}, text='–£—Ä–æ–≤–µ–Ω—å')
                            level = term_element.find_next('div', {
                                'class': 'col b-value'}).text.strip() if term_element else None
                            # –ü–∞—Ä—Å–∏–º –∑–Ω–∞–Ω–∏–µ —è–∑—ã–∫–æ–≤, –µ—Å–ª–∏ –µ—Å—Ç—å
                            lang = None
                            try:
                                lang = vac.find('div', {'class': 'col b-term'}, text='–ê–Ω–≥–ª–∏–π—Å–∫–∏–π')
                                if lang is not None:
                                    level_lang = lang.find_next('span', {
                                        'class': 'b-language-description d-md-none'}).text.strip()
                                    lang = lang.text.strip()
                                    language = f"{lang}: {level_lang}"
                                    if level == level_lang:
                                        level = None
                                else:
                                    language = None
                            except:
                                pass
                            # –ü–∞—Ä—Å–∏–º —Ñ–æ—Ä–º–∞—Ç —Ä–∞–±–æ—Ç—ã
                            job_form_classes = ['g-label-linen', 'g-label-zanah', 'ng-star-inserted']
                            job_form = vac.find('span', class_=job_form_classes)
                            job_format = job_form.get_text(strip=True) if job_form is not None else None

                            # –ü–∞—Ä—Å–∏–º —Å–∫–∏–ª–ª—ã
                            stack_container = vac.find('div', class_='b-vacancy-stack-container')
                            if stack_container is not None:
                                labels = stack_container.find_all('span', class_='g-label')
                                skills = ', '.join([label.text.strip('][') for label in labels])
                            else:
                                skills = None
                            date_of_download = datetime.now().date()
                        except:
                            pass
                        item = {
                            "company": job.get('company', {}).get('name'),
                            "vacancy_name": vacancy_name,
                            "skills": skills,
                            "towns": ', '.join([span.get_text(strip=True).replace('üìç', '') for span in
                                                vac.find_all('span', class_='g-label-secondary')]),
                            "vacancy_url": full_url,
                            "description": description,
                            "job_format": job_format,
                            "level": level,
                            "salary_from": salary_from,
                            "salary_to": salary_to,
                            "date_created": date_created,
                            "date_of_download": date_of_download,
                            "source_vac": 1,
                            "status": 'existing',
                            "version_vac": 1,
                            "actual": 1,
                            "languages": language,
                            "—Åurr_salary_from": —Åurr_salary_from,
                            "—Åurr_salary_to": —Åurr_salary_to,
                            "currency_id": currency_id
                        }
                        print(f"Adding item: {item}")
                        item_df = pd.DataFrame([item])
                        self.df = pd.concat([self.df, item_df], ignore_index=True)
                        time.sleep(3)
                else:
                    print(f"Failed to fetch data for {url}. Status code: {r.status_code}")
                    continue  # –ü—Ä–µ—Ä—ã–≤–∞–µ–º —Ü–∏–∫–ª –ø—Ä–∏ –æ—à–∏–±–∫–µ –∑–∞–ø—Ä–æ—Å–∞
                # i += 1
            self.log.info("–í —Å–ø–∏—Å–æ–∫ –¥–æ–±–∞–≤–ª–µ–Ω—ã –¥–∞–Ω–Ω—ã–µ")

        except AttributeError as e:
            self.log.error(f"Error processing link {url}: {e}")

        self.df = self.df.drop_duplicates()
        self.log.info("Total number of found vacancies after removing duplicates: " + str(len(self.df)) + "\n")
