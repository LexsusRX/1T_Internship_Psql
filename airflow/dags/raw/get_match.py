from airflow.utils.task_group import TaskGroup
import logging
import time
from datetime import datetime
import os
import requests
import pandas as pd
from bs4 import BeautifulSoup
from airflow.utils.dates import days_ago
from psycopg2.extras import execute_values
import re

import sys
import os
sys.path.insert(0, '/opt/airflow/dags/')
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from raw.variables_settings import variables, base_getmatch

table_name = variables['raw_tables'][6]['raw_tables_name']
url = base_getmatch

logging.basicConfig(
    format='%(threadName)s %(name)s %(levelname)s: %(message)s',
    level=logging.INFO
)

log = logging

# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
default_args = {
    "owner": "admin_1T",
    'start_date': days_ago(1)
}

from raw.base_job_parser import BaseJobParser

class GetMatchJobParser(BaseJobParser):
    def find_vacancies(self):
        self.items = []
        HEADERS = {
                    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8",
                    "User-Agent": "Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:98.0) Gecko/20100101 Firefox/98.0",
                }
        self.log.info(f'–°–æ–∑–¥–∞–µ–º –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫')
        self.items = []
        # seen_ids = set()
        self.all_links = []  
        self.log.info(f'–ü–∞—Ä—Å–∏–º –¥–∞–Ω–Ω—ã–µ')
        # –ü–∞—Ä—Å–∏–º –ª–∏–Ω–∫–∏ –≤–∞–∫–∞–Ω—Å–∏–π —Å –∫–∞–∂–¥–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã (–Ω–∞ —Å–∞–π—Ç–µ –ø—Ä–∏–º–µ—Ä–Ω–æ 50—Å—Ç—Ä–∞–Ω–∏—Ü, 100 - —ç—Ç–æ —Å –∏–∑–±—ã—Ç–∫–æ–º)     
        
        for i in range(1, 100):
            response = requests.get(url.format(i=i), headers=HEADERS)
            soup = BeautifulSoup(response.content, 'html.parser')
            divs = soup.find_all('div', class_='b-vacancy-card-title')
            for div in divs:
                vacancy_url = 'https://getmatch.ru/' + div.find('a').get('href')
                self.all_links.append(vacancy_url)

        vacancy_count = 0
        for link in self.all_links:
            if vacancy_count < 15:
                resp = requests.get(link, HEADERS)
                vac = BeautifulSoup(resp.content, 'lxml')

                try:
                    # –ø–∞—Ä—Å–∏–º –≥—Ä–µ–π–¥ –≤–∞–∫–∞–Ω—Å–∏–∏
                    term_element = vac.find('div', {'class': 'col b-term'}, text='–£—Ä–æ–≤–µ–Ω—å')
                    level = term_element.find_next('div', {'class': 'col b-value'}).text.strip() if term_element else None

                    # –ü–∞—Ä—Å–∏–º –∏–Ω–æ—Å—Ç—Ä–∞–Ω–Ω—ã–µ —è–∑—ã–∫–∏
                    lang = vac.find('div', {'class': 'col b-term'}, text='–ê–Ω–≥–ª–∏–π—Å–∫–∏–π')
                    if lang is not None:
                        level_lang= lang.find_next('span', {'class': 'b-language-description d-md-none'}).text.strip()
                        lang=lang.text.strip()
                        language = f"{lang}: {level_lang}"
                        if level==level_lang:
                            level=None
                    else:
                        language=None

                    # –ü–∞—Ä—Å–∏–º –Ω–∞–≤—ã–∫–∏
                    stack_container = vac.find('div', class_='b-vacancy-stack-container')
                    if stack_container is not None:
                        labels = stack_container.find_all('span', class_='g-label')
                        page_stacks = ', '.join([label.text.strip('][') for label in labels])
                    else:
                        page_stacks=None

                    # –ü–∞—Ä—Å–∏–º –æ–ø–∏—Å–∞–Ω–∏–µ –≤–∞–∫–∞–Ω—Å–∏–π
                    description_element = vac.find('section', class_='b-vacancy-description')
                    description_lines = description_element.stripped_strings
                    description = '\n'.join(description_lines)

                    # –ü–∞—Ä—Å–∏–º –∏ —Ä–∞—Å–ø–∞—Ä—à–∏–≤–∞–µ–º –∑–∞—Ä–ø–ª–∞—Ç—ã
                    salary_text = vac.find('h3').text.strip()
                    salary_text = salary_text.replace('\u200d', '-').replace('‚Äî', '-')
                    if '‚ÇΩ/–º–µ—Å –Ω–∞ —Ä—É–∫–∏' in vac.find('h3').text:
                        salary_parts = list(map(str.strip, salary_text.split('-')))
                        salary_from = salary_parts[0]
                        if len(salary_parts) == 1:
                            salary_to = None if '–æ—Ç' in vac.find('h3').text else salary_parts[0]
                        elif len(salary_parts) > 1:
                            salary_to = salary_parts[2]
                    else:
                        salary_from = None
                        salary_to = None

                    # –ü—Ä–∏–≤–æ–¥–∏–º –∑–∞—Ä–ø–ª–∞—Ç—ã –∫ —á–∏—Å–ª–æ–≤–æ–º—É —Ñ–æ—Ä–º–∞—Ç—É
                    if salary_from is not None:
                        numbers = re.findall(r'\d+', salary_from)
                        combined_number = ''.join(numbers)
                        salary_from = int(combined_number) if combined_number else None
                    if salary_to is not None:
                        numbers = re.findall(r'\d+', salary_to)
                        combined_number = ''.join(numbers)
                        salary_to = int(combined_number) if combined_number else None

                    # –ü–∞—Ä—Å–∏–º —Ñ–æ—Ä–º–∞—Ç —Ä–∞–±–æ—Ç—ã
                    job_form_classes = ['g-label-linen', 'g-label-zanah', 'ng-star-inserted']
                    job_form = vac.find('span', class_=job_form_classes)
                    job_format = job_form.get_text(strip=True) if job_form is not None else None

                    # –ü–æ–ª—É—á–∞–µ–º –¥—Ä—É–≥–∏–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
                    date_created = date_of_download = datetime.now().date()
                    status ='existing'
                    version_vac=1
                    actual=1

                    item = {
                        "company": vac.find('h2').text.replace('\xa0', ' ').strip('–≤'),
                        "vacancy_name": vac.find('h1').text,
                        "skills": page_stacks,
                        "towns": ', '.join([span.get_text(strip=True).replace('üìç', '') for span in vac.find_all('span', class_='g-label-secondary')]),
                        "vacancy_url": link,
                        "description": description,
                        "job_format": job_format,
                        "level": level,
                        "salary_from": salary_from,
                        "salary_to": salary_to,
                        "date_created": date_created,
                        "date_of_download": date_of_download,
                        "source_vac": 1,
                        "status": status,
                        "version_vac": version_vac,
                        "actual": actual,
                        "languages":language,
                        }
                    print(f"Adding item: {item}")
                    item_df = pd.DataFrame([item])
                    self.df = pd.concat([self.df, item_df], ignore_index=True)
                    time.sleep(3)
                    vacancy_count += 1
                except AttributeError as e:
                    print(f"Error processing link {link}: {e}")
            else:
                break
        self.df = self.df.drop_duplicates()
        self.log.info("–û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –≤–∞–∫–∞–Ω—Å–∏–π –ø–æ—Å–ª–µ —É–¥–∞–ª–µ–Ω–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: " + str(len(self.df)) + "\n")


# def run_init_getmatch_parser():
#     """
#     –û—Å–Ω–æ–≤–Ω–æ–π –≤–∏–¥ –∑–∞–¥–∞—á–∏ –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –ø–∞—Ä—Å–µ—Ä–∞ –¥–ª—è –≤–∞–∫–∞–Ω—Å–∏–π GetMatch
#     """
#     # log = context['ti'].log
#     log.info('–ó–∞–ø—É—Å–∫ –ø–∞—Ä—Å–µ—Ä–∞ GetMatch')
#     try:
#         parser = GetMatchJobParser(url, log, conn, table_name)
#         parser.find_vacancies()
#         parser.addapt_numpy_null()
#         parser.save_df()
#         log.info('–ü–∞—Ä—Å–µ—Ä GetMatch —É—Å–ø–µ—à–Ω–æ –ø—Ä–æ–≤–µ–ª —Ä–∞–±–æ—Ç—É')
#     except Exception as e:
#         log.error(f'–û—à–∏–±–∫–∞ –≤–æ –≤—Ä–µ–º—è —Ä–∞–±–æ—Ç—ã –ø–∞—Ä—Å–µ—Ä–∞ GetMatch: {e}')
#
#
# with DAG(dag_id="initial_getmatch_parser",
#          schedule_interval=None, tags=['admin_1T'],
#          default_args=default_args,
#          catchup=False) as dag_initial_getmatch_parser:
#     parse_get_match_jobs = PythonOperator(
#         task_id='init_run_getmatch_parser_task',
#         python_callable=run_init_getmatch_parser,
#         provide_context=True)
#
# parse_get_match_jobs
#
# def run_update_getmatch_parser():
#     """
#     –û—Å–Ω–æ–≤–Ω–æ–π –≤–∏–¥ –∑–∞–¥–∞—á–∏ –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –ø–∞—Ä—Å–µ—Ä–∞ –¥–ª—è –≤–∞–∫–∞–Ω—Å–∏–π GetMatch
#     """
#     # log = context['ti'].log
#     log.info('–ó–∞–ø—É—Å–∫ –ø–∞—Ä—Å–µ—Ä–∞ GetMatch')
#     try:
#         parser = GetMatchJobParser(url, log, conn, table_name)
#         parser.find_vacancies()
#         parser.generating_dataframes()
#         parser.addapt_numpy_null()
#         parser.update_database_queries()
#         # parser.stop()
#         log.info('–ü–∞—Ä—Å–µ—Ä GetMatch —É—Å–ø–µ—à–Ω–æ –ø—Ä–æ–≤–µ–ª —Ä–∞–±–æ—Ç—É')
#     except Exception as e:
#         log.error(f'–û—à–∏–±–∫–∞ –≤–æ –≤—Ä–µ–º—è —Ä–∞–±–æ—Ç—ã –ø–∞—Ä—Å–µ—Ä–∞ GetMatch: {e}')
#
# with DAG(dag_id="update_getmatch_parser",
#          schedule_interval=None,
#          tags=['admin_1T'],
#          default_args=default_args,
#          catchup=False) as dag_update_getmatch_parser:
#     update_getmatch_parser_job = PythonOperator(
#         task_id='update_getmatch_parser_task',
#         python_callable=run_update_getmatch_parser,
#         provide_context=True)
#
# dag_update_getmatch_parser