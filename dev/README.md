# Парсинг данных о вакансиях с использованием Apache Airflow, Selenium и PostgreSQL.

В данном проекте реализован процесс получения данных о вакансиях с различных веб-сайтов и их сохранения в базе данных PostgreSQL. Механизм сценария python и расписание автоматизированы с помощью Apache Airflow.

## Описание скрипта.

1. **`create_raw_table`**: Функция, создающая таблицы в базе данных ClickHouse, если они не существует.

2. **`run_*_parser`** : Функции, которые переходят на указанные веб-сайты, собирают информацию о вакансиях и сохраняют ее в PostgreSQL.

Эти задачи определены и выполняются в конкретном порядке в Apache Airflow.

## Запуск проекта с помощью Docker

Для упрощения запуска проекта и обеспечения его воспроизводимости в процессе развертывания приложения использован Docker. Файл `docker-compose.yaml` содержит набор сервисов, необходимых для работы приложения. Для запуска проекта необходимо выполнить команду `docker-compose up -d` в директории с файлом `docker-compose.yml`.

### Сервисы проекта:

- `postgres`: Сервер базы данных PostgreSQL для Airflow.
- `postgres`: Сервер базы данных PostgreSQL для хранения данных вакансий.
- `redis`: Сервер Redis для работы с Airflow.
- `grafana`: Сервер Grafana для визуализации данных.
- `prometheus`: Сервер для сбора метрик.
- `airflow-*`: Набор сервисов, образующих среду выполнения Apache Airflow.
- `selenium-*`: Набор сервисов для выполнения сбора данных с помощью Selenium Grid.
- `chrome`: Сервис для запуска экземпляров браузера Chrome.

### Соединение с DBeaver:

    Создать соединение -> PostgreSQL

    Host: localhost
    Schema: vacancy
    Login: admin
    Password: password
    Port: 10111

